# -*- coding: utf-8 -*-
"""data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vK0A6S2xB6aPdlbAWG-UAGgCpGbUBKEq

DATA.PY
"""

#importing libraries
import os
import numpy as np
import cv2
from glob import glob
import tensorflow as tf
#train_test_split is to spliting dataset in to train, test, vaid
from sklearn.model_selection import train_test_split

# 80 for training, 10 for validation, 10 for testing
def load_data(path, split=0.1):

  #loading image and mask from cvc-612
  #glob is for geeting all the conent available in the file
  #sorting used for arrange image with its repective mask 
  # path="/content/drive/MyDrive/pp1/PROJECT/PROJECT/CVC-612"
    images = sorted(glob(os.path.join(path, "/content/drive/MyDrive/pp1/PROJECT/PROJECT/CVC-612/images/*")))
    masks = sorted(glob(os.path.join(path, "/content/drive/MyDrive/pp1/PROJECT/PROJECT/CVC-612/masks/*")))

#calculating the size of the iamges
    total_size = len(images)
    print
#spliting images in to validsize and testsize    
    valid_size = int(split * total_size)
    test_size = int(split * total_size)
    print(total_size, valid_size, test_size)

#spliting train data into train and validation
    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)
    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)
#spliting training data into train and test
    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)
    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)
    
    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)
#reading the image and converting it to numpy array
def read_image(path):
    path = path.decode()#beacuse the path is in binary format
    x = cv2.imread(path, cv2.IMREAD_COLOR)#geting images into RGB format
    x = cv2.resize(x, (224, 224))
    x = x/255.0 #normalizing the array so that its value will come between 0 and 1, it will help to reduce time.
    # (256,256,3)
    return x

def read_mask(path):
    path = path.decode()
    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)#geting masks in grayscale
    x = cv2.resize(x, (224, 224))
    x = x/255.0 
    #(256,256)
    x = np.expand_dims(x, axis=-1)#expanding dimension of the numpy array.
    #(256,256,1)
    return x  

def tf_parse(x, y):#The tf_parse function parses a single image and mask path.
    def _parse(x, y):
        x = read_image(x)
        y = read_mask(y)
        return x, y

#read_image and read_mask returning numpy array so we have to convert it in to tensorflow usable format
    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])
    x.set_shape([224, 224, 3])
    y.set_shape([224, 224, 1])
    return x, y
#tf.data pipeline which takes a list of images, masks paths and the batch size.
def tf_dataset(x, y, batch=8):
    dataset = tf.data.Dataset.from_tensor_slices((x, y))
    dataset = dataset.map(tf_parse)
    dataset = dataset.batch(batch)#making batch of 8 images
    dataset = dataset.repeat()
    return  dataset

if __name__ =="__main__":
    path="/content/drive/MyDrive/pp1/PROJECT/PROJECT/CVC-612"
    (train_x, train_y), (valid_x, valid_y), (test_x, test_y)= load_data(path) 
    
    #for testing DATA.py
    ds = tf_dataset(test_x,test_y)
    for x,y in ds:
      print(x.shape, y.shape)
      break