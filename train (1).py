# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Z6kBG3joUUn8tbC91nUM2cfUcm9WPSt
"""

from tensorflow.python.ops.gen_dataset_ops import shuffle_dataset
from numpy.random.mtrand import shuffle
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import numpy as np
import cv2
from glob import glob
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard
from tensorflow.keras.metrics import Recall, Precision
#from data import load_data, tf_dataset
#from model import build_model

def iou(y_true, y_pred):#intersection over union
    def f(y_true, y_pred):
        intersection = (y_true * y_pred).sum()
        union = y_true.sum() + y_pred.sum() - intersection
        x = (intersection + 1e-15) / (union + 1e-15)#adding 1e-15 because union's value can be very small
        x = x.astype(np.float32)#converting to float32
        return x
    return tf.numpy_function(f, [y_true, y_pred], tf.float32)

if __name__ == "__main__":

  ##seeding 
    np.random.seed(42)
    tf.random.set_seed(42)
    ## Dataset
    path = "/content/drive/MyDrive/pp1/PROJECT/PROJECT/CVC-612/"
    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(path)

    ## Hyperparameters
    batch = 8
    lr = 1e-4
    epochs = 50

    train_dataset = tf_dataset(train_x, train_y, batch=batch)#training pipe line
    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch)#validating pipeline
   
   #building the u net model
    model = build_model()

    opt = tf.keras.optimizers.Adam(lr)
    metrics = ["acc", Recall(), Precision(), iou]
    model.compile(loss="binary_crossentropy", optimizer=opt, metrics=metrics)#as we want to get loss in binary

    callbacks = [
        ModelCheckpoint("/content/drive/MyDrive/pp1/PROJECT/PROJECT/files/model.h5"),
        #checkpoint= ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True,mode='max')
        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3),#if the validation loss does not reduce after 3 patience gactor will increase by 0.1
        CSVLogger("/content/drive/MyDrive/pp1/PROJECT/PROJECT/files/data.csv"),
        TensorBoard(),
        EarlyStopping(monitor='val_acc', patience=100, restore_best_weights=False)#to avoid overfitting
    ]

    train_steps = len(train_x)//batch#defining number of batches in train_x
    valid_steps = len(valid_x)//batch#defining number of batches in valid_x

    if len(train_x) % batch != 0:#if batch=0 implementing train_steps by 1
        train_steps += 1
    if len(valid_x) % batch != 0:#if batch=0 implementing valid_steps by 1
        valid_steps += 1
        #--patience 0

    model.fit(
        train_dataset,
        validation_data=valid_dataset,
        epochs=epochs,
        steps_per_epoch=train_steps,
        validation_steps=valid_steps,
        callbacks=callbacks
        #shuffle_dataset = False
        )